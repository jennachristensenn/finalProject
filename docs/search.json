[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "The data that I will be working with is the Diabetes Health Indicators Dataset. This is a cleaned dataset of 253,680 responses to the CDC’s Behavioral Risk Factor Surveillance System (BRFSS), a system of health-related telephone surveys. I will be using the following response and predictor variables:\n\nDiabetes_binomial: response variabele [0 = no diabetes, 1 = diabetes]\nSmoker: Have you smoked at least 100 cigarettes in your entire life [0 = no, 1 = yes]\nGenHlth: Would you say your general health is [1 = excellent, 2 = very good, 3 = good, 4 = fair, 5 = poor]\nSex: Gender [0 = female, 1 = male]\nAge: 13 level age category [1 = 18-24, 9 = 60-64, 13 = 80 and older]\nIncome: 8 level income scale [1 = &lt;10,000, 5 = &lt;35,000, 8 = &gt;75,000]\n\nWe examined the dataset in the EDA.qmd file. We want to make sure we have clean data so that our modeling can be as accurate as possible. Here we will explore two different models (classification tree and random forest) by tuning parameters to determine the best model for each type of model. Our goal with modeling is to create an accurate representation of the data, therefore, we will compare which type of model performs the best on the data, and use that to fit the test data. We want to utilize the best possible model so that we can make predictions about future outcomes and identify underlying patterns.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(tree)\n\n\ndiab_data &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndiab_data &lt;- diab_data |&gt;\n  mutate(diab_bin = factor(Diabetes_binary, levels = c(0, 1),\n                           labels = c(\"no diabetes\", \"diabetes\")),\n         smoker = factor(Smoker, levels = c(0, 1), \n                         labels = c(\"no\", \"yes\")),\n         genHlth = factor(GenHlth, levels  = c(1, 2, 3, 4, 5), \n                          labels = c(\"excellent\", \"very good\", \"good\", \"fair\", \"poor\")),\n         sex = factor(Sex, levels = c(0, 1), \n                      labels = c(\"female\", \"male\")),\n         age = factor(Age, levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), \n                      labels = c(\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80+\")),\n         income = factor(Income, levels = c(1, 2, 3, 4, 5, 6, 7, 8),\n                         labels = c(\"&lt;10\", \"10-15\", \"15-20\", \"20-25\", \"25-35\", \"35-50\", \"50-75\", \"75+\"))) |&gt;\n  select(diab_bin, smoker, genHlth, sex, age, income) \ndiab_data\n\n# A tibble: 253,680 × 6\n   diab_bin    smoker genHlth   sex    age   income\n   &lt;fct&gt;       &lt;fct&gt;  &lt;fct&gt;     &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt; \n 1 no diabetes yes    poor      female 60-64 15-20 \n 2 no diabetes yes    good      female 50-54 &lt;10   \n 3 no diabetes no     poor      female 60-64 75+   \n 4 no diabetes no     very good female 70-74 35-50 \n 5 no diabetes no     very good female 70-74 20-25 \n 6 no diabetes yes    very good male   65-69 75+   \n 7 no diabetes yes    good      female 60-64 50-75 \n 8 no diabetes yes    good      female 70-74 20-25 \n 9 diabetes    yes    poor      female 60-64 &lt;10   \n10 no diabetes no     very good male   55-59 15-20 \n# ℹ 253,670 more rows\n\n\nSplitting the data into training and test sets.\n\nset.seed(10)\ndiab_split &lt;- initial_split(diab_data, prop = 0.7)\ndiab_train &lt;- training(diab_split)\ndiab_test &lt;- testing(diab_split)\ndiab_5_fold &lt;- vfold_cv(diab_train, 5)\n\nDefining the recipe that will be used for both models.\n\nrec &lt;- recipe(diab_bin ~ ., data = diab_data) |&gt;\n  step_dummy(smoker, genHlth, sex, age, income)\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 5\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: smoker, genHlth, sex, age, income"
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "Modeling",
    "section": "",
    "text": "The data that I will be working with is the Diabetes Health Indicators Dataset. This is a cleaned dataset of 253,680 responses to the CDC’s Behavioral Risk Factor Surveillance System (BRFSS), a system of health-related telephone surveys. I will be using the following response and predictor variables:\n\nDiabetes_binomial: response variabele [0 = no diabetes, 1 = diabetes]\nSmoker: Have you smoked at least 100 cigarettes in your entire life [0 = no, 1 = yes]\nGenHlth: Would you say your general health is [1 = excellent, 2 = very good, 3 = good, 4 = fair, 5 = poor]\nSex: Gender [0 = female, 1 = male]\nAge: 13 level age category [1 = 18-24, 9 = 60-64, 13 = 80 and older]\nIncome: 8 level income scale [1 = &lt;10,000, 5 = &lt;35,000, 8 = &gt;75,000]\n\nWe examined the dataset in the EDA.qmd file. We want to make sure we have clean data so that our modeling can be as accurate as possible. Here we will explore two different models (classification tree and random forest) by tuning parameters to determine the best model for each type of model. Our goal with modeling is to create an accurate representation of the data, therefore, we will compare which type of model performs the best on the data, and use that to fit the test data. We want to utilize the best possible model so that we can make predictions about future outcomes and identify underlying patterns.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(tree)\n\n\ndiab_data &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndiab_data &lt;- diab_data |&gt;\n  mutate(diab_bin = factor(Diabetes_binary, levels = c(0, 1),\n                           labels = c(\"no diabetes\", \"diabetes\")),\n         smoker = factor(Smoker, levels = c(0, 1), \n                         labels = c(\"no\", \"yes\")),\n         genHlth = factor(GenHlth, levels  = c(1, 2, 3, 4, 5), \n                          labels = c(\"excellent\", \"very good\", \"good\", \"fair\", \"poor\")),\n         sex = factor(Sex, levels = c(0, 1), \n                      labels = c(\"female\", \"male\")),\n         age = factor(Age, levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), \n                      labels = c(\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80+\")),\n         income = factor(Income, levels = c(1, 2, 3, 4, 5, 6, 7, 8),\n                         labels = c(\"&lt;10\", \"10-15\", \"15-20\", \"20-25\", \"25-35\", \"35-50\", \"50-75\", \"75+\"))) |&gt;\n  select(diab_bin, smoker, genHlth, sex, age, income) \ndiab_data\n\n# A tibble: 253,680 × 6\n   diab_bin    smoker genHlth   sex    age   income\n   &lt;fct&gt;       &lt;fct&gt;  &lt;fct&gt;     &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt; \n 1 no diabetes yes    poor      female 60-64 15-20 \n 2 no diabetes yes    good      female 50-54 &lt;10   \n 3 no diabetes no     poor      female 60-64 75+   \n 4 no diabetes no     very good female 70-74 35-50 \n 5 no diabetes no     very good female 70-74 20-25 \n 6 no diabetes yes    very good male   65-69 75+   \n 7 no diabetes yes    good      female 60-64 50-75 \n 8 no diabetes yes    good      female 70-74 20-25 \n 9 diabetes    yes    poor      female 60-64 &lt;10   \n10 no diabetes no     very good male   55-59 15-20 \n# ℹ 253,670 more rows\n\n\nSplitting the data into training and test sets.\n\nset.seed(10)\ndiab_split &lt;- initial_split(diab_data, prop = 0.7)\ndiab_train &lt;- training(diab_split)\ndiab_test &lt;- testing(diab_split)\ndiab_5_fold &lt;- vfold_cv(diab_train, 5)\n\nDefining the recipe that will be used for both models.\n\nrec &lt;- recipe(diab_bin ~ ., data = diab_data) |&gt;\n  step_dummy(smoker, genHlth, sex, age, income)\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 5\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: smoker, genHlth, sex, age, income"
  },
  {
    "objectID": "Modeling.html#classification-tree-model",
    "href": "Modeling.html#classification-tree-model",
    "title": "Modeling",
    "section": "Classification Tree Model",
    "text": "Classification Tree Model\nA classification tree is a predictive model that attempts to classify data into categories. The algorithm splits the data into smaller subsets to predict the outcome of different variables. When using classification trees, the model only produces one tree which can sometimes lead to overfitting or a poor generalization of the data. The flip side of this is that they are easy to interpret and clearly display the decision making process.\nDefining the model and engine.\n\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 40,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\nCreating the workflow.\n\ntree_wfl &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(tree_mod)\n\nUsing CV to select the tuning parameters.\n\ntree_fit &lt;- tree_wfl |&gt;\n  tune_grid(resamples = diab_5_fold,\n            grid = 5,\n            metrics = metric_set(mn_log_loss))\n\nWarning: package 'rpart' was built under R version 4.2.3\n\ntree_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  arrange(mean)\n\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric     .estimator  mean     n  std_err .config\n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;  \n1   0.00000000394         14 mn_log_loss binary     0.373     5 0.00340  Prepro…\n2   0.000000782            9 mn_log_loss binary     0.377     5 0.00107  Prepro…\n3   0.0322                 5 mn_log_loss binary     0.404     5 0.000720 Prepro…\n4   0.0000000720           2 mn_log_loss binary     0.404     5 0.000720 Prepro…\n5   0.000385              11 mn_log_loss binary     0.404     5 0.000720 Prepro…\n\n\nSelecting the best tuning parameter.\n\ntree_best &lt;- select_best(tree_fit, metric = \"mn_log_loss\")\ntree_best\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config             \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;               \n1   0.00000000394         14 Preprocessor1_Model4\n\n\nFitting on the entire training set.\n\ntree_final_wfl &lt;- tree_wfl |&gt;\n  finalize_workflow(tree_best)"
  },
  {
    "objectID": "Modeling.html#random-forest-model",
    "href": "Modeling.html#random-forest-model",
    "title": "Modeling",
    "section": "Random Forest Model",
    "text": "Random Forest Model\nA random forest model is an ensemble model that utilizes bootstrap samples. This model has potential to fit the data better than a basic classification tree because it takes the average of multiple trees that the model creates. In the context of classification, the model attempts to predict group membership with the most prevalent class in a region. Additionally, random forest models don’t use every predictor at each step, and instead explores the combinations of random subsets of predictors to determine the best fit.\nDefining the model and engine.\n\nrf_mod &lt;- rand_forest(mtry = tune()) |&gt;\n  set_engine(\"ranger\", importance = \"impurity\") |&gt;\n  set_mode(\"classification\")\n\nCreating the workflow.\n\nrf_wfl &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(rf_mod)\n\nFitting to CV folds and examining metrics.\n\nrf_fit &lt;- rf_wfl |&gt;\n  tune_grid(resamples = diab_5_fold,\n            grid = 5,\n            metrics = metric_set(mn_log_loss))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWarning: package 'ranger' was built under R version 4.2.3\n\nrf_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  arrange(mean)\n\n# A tibble: 5 × 7\n   mtry .metric     .estimator  mean     n  std_err .config             \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1     6 mn_log_loss binary     0.343     5 0.000640 Preprocessor1_Model5\n2    14 mn_log_loss binary     0.345     5 0.000902 Preprocessor1_Model4\n3    16 mn_log_loss binary     0.346     5 0.000990 Preprocessor1_Model2\n4     2 mn_log_loss binary     0.360     5 0.000515 Preprocessor1_Model3\n5    25 mn_log_loss binary     0.383     5 0.00411  Preprocessor1_Model1\n\n\nSelecting the best tuning parameter\n\nrf_best &lt;- select_best(rf_fit, metric = \"mn_log_loss\")\nrf_best\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1     6 Preprocessor1_Model5\n\n\nFitting on the entire training set.\n\nrf_final_wfl &lt;- rf_wfl |&gt;\n  finalize_workflow(rf_best)"
  },
  {
    "objectID": "Modeling.html#final-model-selection",
    "href": "Modeling.html#final-model-selection",
    "title": "Modeling",
    "section": "Final Model Selection",
    "text": "Final Model Selection\nExamining how the final fitted models perform on the test set.\n\ntree_final_fit &lt;- tree_final_wfl |&gt;\n  last_fit(diab_split, metrics = metric_set(mn_log_loss))\n\ntree_final_fit |&gt; collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.377 Preprocessor1_Model1\n\nrf_final_fit &lt;- rf_final_wfl |&gt;\n  last_fit(diab_split, metrics = metric_set(mn_log_loss))\n\nrf_final_fit |&gt; collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.344 Preprocessor1_Model1\n\n\nThe overall best fit comes from the Random Forest model as the log loss metric is lower (0.377 for CT and 0.344 for RF). We will be using this model to fit to the entire dataset.\nExamining Variable importance.\n\nrf_final_model &lt;- extract_fit_engine(rf_final_fit)\ntibble(term = names(rf_final_model$variable.importance),\n       value = rf_final_model$variable.importance) |&gt;\n  arrange(value)|&gt;\n  mutate(term = factor(term, levels = term)) |&gt;\n  ggplot(aes(x = term, y = value)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip()"
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "The data that I will be working with is the Diabetes Health Indicators Dataset. This is a cleaned dataset of 253,680 responses to the CDC’s Behavioral Risk Factor Surveillance System (BRFSS), a system of health-related telephone surveys. The data includes the response variable ‘Diabetes_binary’ indicating that a respondent either has diabetes or prediabetes [1], or does not have diabetes [0]. Twenty-one other variables are available that include both qualitative and quantitative variables. This data is commonly used to answer questions such as “which variables are accurate predictors of whether an individual has diabetes.” In addition to the response variable ‘Diabetes_binary’ I will be using the following predictor variables for my analysis:\n\nSmoker: Have you smoked at least 100 cigarettes in your entire life [0 = no, 1 = yes]\nGenHlth: Would you say your general health is [1 = excellent, 2 = very good, 3 = good, 4 = fair, 5 = poor]\nSex: Gender [0 = female, 1 = male]\nAge: 13 level age category [1 = 18-24, 9 = 60-64, 13 = 80 and older]\nIncome: 8 level income scale [1 = &lt;10,000, 5 = &lt;35,000, 8 = &gt;75,000]\n\nThe goal of exploratory data analysis is to examine the underlying data. This is done by making sure each variable is assigned their appropriate category (dlb, char, factor), examining numeric summaries of the variables, and exploring different plots to visually inspect the data. Another aspect to check is missing values, as this can impact results calculated from the data. We want to make sure we have clean data so that our modeling can be as accurate as possible. For this project, our goal with modeling is to create an accurate representation of the data contained in the given set. We want to utilize the best possible model so that we can make predictions about future outcomes and identify underlying patterns. To achieve this, we train parameters and explore different types of models to get best representation of the data."
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "EDA",
    "section": "",
    "text": "The data that I will be working with is the Diabetes Health Indicators Dataset. This is a cleaned dataset of 253,680 responses to the CDC’s Behavioral Risk Factor Surveillance System (BRFSS), a system of health-related telephone surveys. The data includes the response variable ‘Diabetes_binary’ indicating that a respondent either has diabetes or prediabetes [1], or does not have diabetes [0]. Twenty-one other variables are available that include both qualitative and quantitative variables. This data is commonly used to answer questions such as “which variables are accurate predictors of whether an individual has diabetes.” In addition to the response variable ‘Diabetes_binary’ I will be using the following predictor variables for my analysis:\n\nSmoker: Have you smoked at least 100 cigarettes in your entire life [0 = no, 1 = yes]\nGenHlth: Would you say your general health is [1 = excellent, 2 = very good, 3 = good, 4 = fair, 5 = poor]\nSex: Gender [0 = female, 1 = male]\nAge: 13 level age category [1 = 18-24, 9 = 60-64, 13 = 80 and older]\nIncome: 8 level income scale [1 = &lt;10,000, 5 = &lt;35,000, 8 = &gt;75,000]\n\nThe goal of exploratory data analysis is to examine the underlying data. This is done by making sure each variable is assigned their appropriate category (dlb, char, factor), examining numeric summaries of the variables, and exploring different plots to visually inspect the data. Another aspect to check is missing values, as this can impact results calculated from the data. We want to make sure we have clean data so that our modeling can be as accurate as possible. For this project, our goal with modeling is to create an accurate representation of the data contained in the given set. We want to utilize the best possible model so that we can make predictions about future outcomes and identify underlying patterns. To achieve this, we train parameters and explore different types of models to get best representation of the data."
  },
  {
    "objectID": "EDA.html#data",
    "href": "EDA.html#data",
    "title": "EDA",
    "section": "Data",
    "text": "Data\nReading in the diabetes dataset.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.2.3\n\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'readr' was built under R version 4.2.3\n\n\nWarning: package 'purrr' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.3\n\n\nWarning: package 'forcats' was built under R version 4.2.3\n\n\nWarning: package 'lubridate' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndiab_data &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndiab_data\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1               0      1        1         1    40      1      0\n 2               0      0        0         0    25      1      0\n 3               0      1        1         1    28      0      0\n 4               0      1        0         1    27      0      0\n 5               0      1        1         1    24      0      0\n 6               0      1        1         1    25      1      0\n 7               0      1        0         1    30      1      0\n 8               0      1        1         1    25      1      0\n 9               1      1        1         1    30      1      0\n10               0      0        0         1    24      0      0\n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\nSubsetting the data, converting variables where necessary, and assigning appropriate labels.\n\ndiab_data &lt;- diab_data |&gt;\n  mutate(diab_bin = factor(Diabetes_binary, levels = c(0, 1),\n                           labels = c(\"no diabetes\", \"diabetes\")),\n         smoker = factor(Smoker, levels = c(0, 1), \n                         labels = c(\"no\", \"yes\")),\n         genHlth = factor(GenHlth, levels  = c(1, 2, 3, 4, 5), \n                          labels = c(\"excellent\", \"very good\", \"good\", \"fair\", \"poor\")),\n         sex = factor(Sex, levels = c(0, 1), \n                      labels = c(\"female\", \"male\")),\n         age = factor(Age, levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), \n                      labels = c(\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80+\")),\n         income = factor(Income, levels = c(1, 2, 3, 4, 5, 6, 7, 8),\n                         labels = c(\"&lt;10\", \"10-15\", \"15-20\", \"20-25\", \"25-35\", \"35-50\", \"50-75\", \"75+\"))) |&gt;\n  select(diab_bin, smoker, genHlth, sex, age, income) \ndiab_data\n\n# A tibble: 253,680 × 6\n   diab_bin    smoker genHlth   sex    age   income\n   &lt;fct&gt;       &lt;fct&gt;  &lt;fct&gt;     &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt; \n 1 no diabetes yes    poor      female 60-64 15-20 \n 2 no diabetes yes    good      female 50-54 &lt;10   \n 3 no diabetes no     poor      female 60-64 75+   \n 4 no diabetes no     very good female 70-74 35-50 \n 5 no diabetes no     very good female 70-74 20-25 \n 6 no diabetes yes    very good male   65-69 75+   \n 7 no diabetes yes    good      female 60-64 50-75 \n 8 no diabetes yes    good      female 70-74 20-25 \n 9 diabetes    yes    poor      female 60-64 &lt;10   \n10 no diabetes no     very good male   55-59 15-20 \n# ℹ 253,670 more rows\n\n\nInvestigating missing values, there do not appear to be any in the variables which makes sense because the owener of the dataset removed them.\n\nsum_na &lt;- function(column){\n sum(is.na(column))\n}\nna_counts &lt;- diab_data |&gt;\n summarize(across(everything(), sum_na))\n\nna_counts\n\n# A tibble: 1 × 6\n  diab_bin smoker genHlth   sex   age income\n     &lt;int&gt;  &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1        0      0       0     0     0      0"
  },
  {
    "objectID": "EDA.html#summarizations",
    "href": "EDA.html#summarizations",
    "title": "EDA",
    "section": "Summarizations",
    "text": "Summarizations\n** Might want to split up Looking into contingency tables to get counts of different groupings of variables. Looking through these values we notice some interesting trends relating the response variable to general health and age. The gap between diabetes and no diabetes numbers gets smaller as health score declines. With the age variable we notice increasing numbers of responses with diabetes as age increases.\n\ndiab_ct &lt;- table(diab_data$diab_bin)\ndiab_ct\n\n\nno diabetes    diabetes \n     218334       35346 \n\ndiab_smoker &lt;- table(diab_data$diab_bin, diab_data$smoker)\ndiab_smoker\n\n             \n                  no    yes\n  no diabetes 124228  94106\n  diabetes     17029  18317\n\ndiab_hlth &lt;- table(diab_data$diab_bin, diab_data$genHlth)\ndiab_hlth\n\n             \n              excellent very good  good  fair  poor\n  no diabetes     44159     82703 62189 21780  7503\n  diabetes         1140      6381 13457  9790  4578\n\ndiab_sex &lt;- table(diab_data$diab_bin, diab_data$sex)\ndiab_sex\n\n             \n              female   male\n  no diabetes 123563  94771\n  diabetes     18411  16935\n\ndiab_age &lt;- table(diab_data$diab_bin, diab_data$age)\ndiab_age\n\n             \n              18-24 25-29 30-34 35-39 40-44 45-49 50-54 55-59 60-64 65-69 70-74\n  no diabetes  5622  7458 10809 13197 15106 18077 23226 26569 27511 25636 18392\n  diabetes       78   140   314   626  1051  1742  3088  4263  5733  6558  5141\n             \n              75-79   80+\n  no diabetes 12577 14154\n  diabetes     3403  3209\n\ndiab_inc &lt;- table(diab_data$diab_bin, diab_data$income)\ndiab_inc\n\n             \n                &lt;10 10-15 15-20 20-25 25-35 35-50 50-75   75+\n  no diabetes  7428  8697 12426 16081 21379 31179 37954 83190\n  diabetes     2383  3086  3568  4054  4504  5291  5265  7195\n\n\nIn this plot we see the number of responses that have diabetes or not grouped by their general health classification split between males and females. We see that the groups withing the has diabetes category appear similar between males and females\n\nggplot(diab_data, aes(x = diab_bin)) +\n  geom_bar(aes(color = genHlth)) +\n  facet_wrap(~sex)\n\n\n\n\n\n\n\n\nSimilarly, here we see the breakdown of income between smokers and non smokers to be relatively similar between people with and without diabetes.\n\nggplot(diab_data, aes(x = diab_bin)) +\n  geom_bar(aes(color = income)) +\n  facet_wrap(~smoker)\n\n\n\n\n\n\n\n\nClick here for the modeling page"
  }
]