---
title: "Modeling"
format: html
editor: visual
---

## Introduction

The data that I will be working with is the Diabetes Health Indicators Dataset. This is a cleaned dataset of 253,680 responses to the CDC's Behavioral Risk Factor Surveillance System (BRFSS), a system of health-related telephone surveys. I will be using the following response and predictor variables:

- Diabetes_binomial: response variabele [0 = no diabetes, 1 = diabetes]

- BMI: Body Mass Index (Numeric)

- Smoker: Have you smoked at least 100 cigarettes in your entire life [0 = no, 1 = yes]

- GenHlth: Would you say your general health is [1 = excellent, 2 = very good, 3 = good, 4 = fair, 5 = poor]

- Sex: Gender [0 = female, 1 = male]

- Age: 13 level age category [1 = 18-24, 9 = 60-64, 13 = 80 and older]

- Income: 8 level income scale [1 = <10,000, 5 = <35,000, 8 = >75,000]

We examined the dataset in the EDA.qmd file. We want to make sure we have clean data so that our modeling can be as accurate as possible. Here we will explore two different models (classification tree and random forest) by tuning parameters to determine the best model for each type of model. Our goal with modeling is to create an accurate representation of the data, therefore, we will compare which type of model performs the best on the data, and use that to fit the test data. We want to utilize the best possible model so that we can make predictions about future outcomes and identify underlying patterns.

```{r}
library(tidyverse)
library(tidymodels)
library(tree)
```

```{r}
diab_data <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv")

diab_data <- diab_data |>
  mutate(diab_bin = factor(Diabetes_binary, levels = c(0, 1),
                           labels = c("no diabetes", "diabetes")),
         smoker = factor(Smoker, levels = c(0, 1), 
                         labels = c("no", "yes")),
         genHlth = factor(GenHlth, levels  = c(1, 2, 3, 4, 5), 
                          labels = c("excellent", "very good", "good", "fair", "poor")),
         sex = factor(Sex, levels = c(0, 1), 
                      labels = c("female", "Male")),
         age = factor(Age, levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), 
                      labels = c("18-24", "25-29", "30-34", "35-39", "40-44", "45-49", "50-54", "55-59", "60-64", "65-69", "70-74", "75-79", "80+")),
         income = factor(Income, levels = c(1, 2, 3, 4, 5, 6, 7, 8),
                         labels = c("<10", "10-15", "15-20", "20-25", "25-35", "35-50", "50-75", "75+"))) |>
  select(diab_bin, BMI, smoker, genHlth, sex, age, income) |>
  rename("bmi" = "BMI")
diab_data
```

Splitting the data into training and test sets.
```{r}
set.seed(10)
diab_split <- initial_split(diab_data, prop = 0.7)
diab_train <- training(diab_split)
diab_test <- testing(diab_split)
diab_5_fold <- vfold_cv(diab_train, 5)
```

## Classification Tree Model

A classification tree is a predictive model that attempts to split up predictor space into regions 

```{r}
rec <- recipe(diab_bin ~ ., data = diab_data) |>
  step_normalize(bmi) |>
  step_dummy(smoker, genHlth, sex, age, income)
rec
```

Defining the model and engine.
```{r}
tree_mod <- decision_tree(tree_depth = tune(),
                          min_n = 40,
                          cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")
```

Creating the workflow. 
```{r}
tree_wfl <- workflow() |>
  add_recipe(rec) |>
  add_model(tree_mod)
```

Using CV to select the tuning parameters... **figure out if I need to use tune_grid() and grid_regular()
```{r}
tree_fit <- tree_wfl |>
  tune_grid(resamples = diab_5_fold,
            grid = 5,
            metrics = metric_set(mn_log_loss))
tree_fit |>
  collect_metrics() |>
  filter(.metric == "mn_log_loss") |>
  arrange(mean)
```

## Random Forest Model

Defining the model and engine.
```{r}
rf_mod <- rand_forest(mtry = tune()) |>
  set_engine("ranger") |>
  set_mode("classification")
```

Creating the workflow.
```{r}
rf_wfl <- workflow() |>
  add_recipe(rec) |>
  add_model(rf_mod)
```

Fitting to CV folds.
```{r}
rf_fit <- rf_wfl |>
  tune_grid(resamples = diab_5_fold,
            grid = 5,
            metrics = metric_set(mn_log_loss))
```

Examining the metrics.
```{r}
rf_fit |>
  collect_metrics() |>
  filter(.metric == "mn_log_loss") |>
  arrange(mean)
```

