---
title: "Modeling"
format: html
editor: visual
---

## Introduction

The data that I will be working with is the Diabetes Health Indicators Dataset. This is a cleaned dataset of 253,680 responses to the CDC's Behavioral Risk Factor Surveillance System (BRFSS), a system of health-related telephone surveys. I will be using the following response and predictor variables:

- Diabetes_binomial: response variabele [0 = no diabetes, 1 = diabetes]

- BMI: Body Mass Index (Numeric)

- Smoker: Have you smoked at least 100 cigarettes in your entire life [0 = no, 1 = yes]

- GenHlth: Would you say your general health is [1 = excellent, 2 = very good, 3 = good, 4 = fair, 5 = poor]

- Sex: Gender [0 = female, 1 = male]

- Age: 13 level age category [1 = 18-24, 9 = 60-64, 13 = 80 and older]

- Income: 8 level income scale [1 = <10,000, 5 = <35,000, 8 = >75,000]

We examined the dataset in the EDA.qmd file. We want to make sure we have clean data so that our modeling can be as accurate as possible. Here we will explore two different models (classification tree and random forest) by tuning parameters to determine the best model for each type of model. Our goal with modeling is to create an accurate representation of the data, therefore, we will compare which type of model performs the best on the data, and use that to fit the test data. We want to utilize the best possible model so that we can make predictions about future outcomes and identify underlying patterns.

```{r, warning = FLASE}
library(tidyverse)
library(tidymodels)
library(tree)
```

```{r}
diab_data <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv")

diab_data <- diab_data |>
  mutate(diab_bin = factor(Diabetes_binary, levels = c(0, 1),
                           labels = c("no diabetes", "diabetes")),
         smoker = factor(Smoker, levels = c(0, 1), 
                         labels = c("no", "yes")),
         genHlth = factor(GenHlth, levels  = c(1, 2, 3, 4, 5), 
                          labels = c("excellent", "very good", "good", "fair", "poor")),
         sex = factor(Sex, levels = c(0, 1), 
                      labels = c("female", "Male")),
         age = factor(Age, levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), 
                      labels = c("18-24", "25-29", "30-34", "35-39", "40-44", "45-49", "50-54", "55-59", "60-64", "65-69", "70-74", "75-79", "80+")),
         income = factor(Income, levels = c(1, 2, 3, 4, 5, 6, 7, 8),
                         labels = c("<10", "10-15", "15-20", "20-25", "25-35", "35-50", "50-75", "75+"))) |>
  select(diab_bin, BMI, smoker, genHlth, sex, age, income) |>
  rename("bmi" = "BMI")
diab_data
```

Splitting the data into training and test sets.
```{r}
set.seed(10)
diab_split <- initial_split(diab_data, prop = 0.7)
diab_train <- training(diab_split)
diab_test <- testing(diab_split)
diab_5_fold <- vfold_cv(diab_train, 5)
```

## Classification Tree Model

A classification tree is a predictive model that attempts to split up predictor space into regions. 

```{r}
rec <- recipe(diab_bin ~ ., data = diab_data) |>
  step_normalize(bmi) |>
  step_dummy(smoker, genHlth, sex, age, income)
rec
```

Defining the model and engine.
```{r}
tree_mod <- decision_tree(tree_depth = tune(),
                          min_n = 40,
                          cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")
```

Creating the workflow. 
```{r}
tree_wfl <- workflow() |>
  add_recipe(rec) |>
  add_model(tree_mod)
```

Using CV to select the tuning parameters.
```{r}
tree_fit <- tree_wfl |>
  tune_grid(resamples = diab_5_fold,
            grid = 5,
            metrics = metric_set(mn_log_loss))
tree_fit |>
  collect_metrics() |>
  filter(.metric == "mn_log_loss") |>
  arrange(mean)
```

Selecting the best tuning parameter.
```{r}
tree_best <- select_best(tree_fit, metric = "mn_log_loss")
tree_best
```

Fitting on the entire training set.
```{r}
tree_final_wfl <- tree_wfl |>
  finalize_workflow(tree_best)
```

## Random Forest Model

A random forest model is an ensemble model that utilizes bootstrap samples. This model has potential to fit a model better than a basic classification tree because it takes the average of multiple trees that the model creates. In the context of classification, the model attempts to predict group membership with the most prevalent class in a region. Additionally, random forest models don't use every predictor at each step, and instead explores the combinations of random subsets of predictors to determine the best fit. 

Defining the model and engine.
```{r}
rf_mod <- rand_forest(mtry = tune()) |>
  set_engine("ranger") |>
  set_mode("classification")
```

Creating the workflow.
```{r}
rf_wfl <- workflow() |>
  add_recipe(rec) |>
  add_model(rf_mod)
```

Fitting to CV folds and examining metrics.
```{r}
rf_fit <- rf_wfl |>
  tune_grid(resamples = diab_5_fold,
            grid = 5,
            metrics = metric_set(mn_log_loss))

rf_fit |>
  collect_metrics() |>
  filter(.metric == "mn_log_loss") |>
  arrange(mean)
```

Selecting the best tuning parameter
```{r}
rf_best <- select_best(rf_fit, metric = "mn_log_loss")
rf_best
```

Fitting on the entire training set.
```{r}
rf_final_wfl <- rf_wfl |>
  finalize_workflow(rf_best)
```

## Final Model Selection

Examining how the final fitted models perform on the test set. 
```{r}
tree_final_fit <- tree_final_wfl |>
  last_fit(diab_split, metrics = metric_set(mn_log_loss))

tree_final_fit |> collect_metrics()

rf_final_fit <- rf_final_wfl |>
  last_fit(diab_split, metrics = metric_set(mn_log_loss))

rf_final_fit |> collect_metrics()
```

